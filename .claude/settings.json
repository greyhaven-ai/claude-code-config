{
  "permissions": {
    "allow": [
      "mcp__firecrawl-mcp__firecrawl_scrape"
    ],
    "deny": [],
    "ask": []
  },
  "hooks": {
    "PreToolUse": [
      {
        "name": "destructive-operation-validator",
        "description": "Prevents destructive operations without confirmation (Phase 1 - Critical Safety)",
        "hooks": [
          {
            "type": "prompt",
            "prompt": "You are a safety validation expert evaluating tool usage for potential destructive operations.\n\nAnalyze this tool use request:\n\nTool: $ARGUMENTS.toolName\nArguments: $ARGUMENTS.arguments\nFile path: $ARGUMENTS.filePath (if applicable)\nCurrent working directory: $ARGUMENTS.cwd\n\nEvaluate for these HIGH RISK patterns:\n\n1. **Git Force Operations**\n   - git push --force, git push -f\n   - Especially to main/master branches\n   - Decision: BLOCK with confirmation request\n\n2. **File/Directory Deletion**\n   - rm -rf, rmdir on critical directories\n   - Deletion of .git, node_modules, dist, build (OK)\n   - Deletion of src/, config/, production files (BLOCK)\n   - Decision: BLOCK for critical dirs, APPROVE for build artifacts\n\n3. **Database Operations**\n   - DROP DATABASE, TRUNCATE, DELETE without WHERE\n   - Production database connections\n   - Decision: BLOCK on production, APPROVE on dev/test\n\n4. **Production Environment**\n   - Modifications to files containing 'production', 'prod', 'prd'\n   - Changes to .env.production, config/production.*\n   - Deployment scripts\n   - Decision: BLOCK with explicit confirmation\n\n5. **Security Files**\n   - Deletion or disabling of auth/security code\n   - Changes to .github/workflows (CI/CD)\n   - Modifications to Dockerfile, docker-compose.yml\n   - Decision: APPROVE but add WARNING\n\n**Response Format**:\n\nFor HIGH RISK (force push to main, production changes, destructive DB ops):\n{\n  \"decision\": \"block\",\n  \"reason\": \"Destructive operation detected: [specific operation]\",\n  \"systemMessage\": \"âš ï¸ DESTRUCTIVE OPERATION\\n\\n[Operation details]\\n\\nThis operation could cause data loss or production issues.\\n\\nIf this is intentional, please:\\n1. [Specific mitigation steps]\\n2. Confirm by re-running with explicit flag\\n\\nOr modify your approach to avoid this risk.\"\n}\n\nFor MEDIUM RISK (security file changes, deployment configs):\n{\n  \"decision\": \"approve\",\n  \"systemMessage\": \"âš ï¸ CAUTION: Modifying [security/deployment] files. Ensure changes are reviewed.\"\n}\n\nFor LOW RISK (normal operations, build artifact cleanup):\n{\n  \"decision\": \"approve\"\n}\n\nNow evaluate the provided tool use request and respond with JSON only.",
            "timeout": 25
          }
        ],
        "toolNames": ["Bash", "Write", "Edit", "MultiEdit"]
      },
      {
        "name": "security-file-protection",
        "description": "Protects security-critical files from unauthorized modification (Phase 1 - Critical Safety)",
        "hooks": [
          {
            "type": "prompt",
            "prompt": "You are a security file protection expert. Evaluate if this file modification affects security.\n\nFile path: $ARGUMENTS.filePath\nOperation: $ARGUMENTS.toolName\nChange type: [analyze from context if available]\n\n**Security-Sensitive File Patterns**:\n\n1. **Authentication/Authorization**\n   - Patterns: **/auth*.{js,ts,py}, **/login*.{js,ts,py}, **/session*.{js,ts,py}\n   - Patterns: **/jwt*.{js,ts,py}, **/oauth*.{js,ts,py}, **/permission*.{js,ts,py}\n   - Risk: HIGH if disabling checks, MEDIUM if refactoring\n\n2. **Secrets/Configuration**\n   - Patterns: **/.env*, **/secrets*, **/credentials*, **/config/production*\n   - Patterns: **/keys/**, **/*.pem, **/*.key\n   - Risk: CRITICAL - should never be directly edited\n\n3. **Cryptography**\n   - Patterns: **/crypto*.{js,ts,py}, **/hash*.{js,ts,py}, **/encrypt*.{js,ts,py}\n   - Risk: HIGH if changing algorithms, MEDIUM if adding features\n\n4. **Infrastructure**\n   - Patterns: Dockerfile, docker-compose*.yml, **/*.k8s.yaml\n   - Patterns: .github/workflows/**, **/.circleci/**, **/.gitlab-ci.yml\n   - Risk: MEDIUM - changes affect deployment/security\n\n5. **Input Validation**\n   - Patterns: **/validation*.{js,ts,py}, **/sanitize*.{js,ts,py}\n   - Risk: HIGH if removing validation, LOW if adding\n\n**Decision Logic**:\n\nCRITICAL RISK (secrets, credentials):\n{\n  \"decision\": \"block\",\n  \"reason\": \"Direct modification of secrets/credentials file detected\",\n  \"systemMessage\": \"ðŸ”’ SECURITY: Secrets files should not be directly edited.\\n\\nUse environment variables or secret management system instead.\\n\\nFile: [path]\\n\\nIf you must edit, use: vault edit [file]\"\n}\n\nHIGH RISK (disabling auth, weakening crypto):\n{\n  \"decision\": \"block\",\n  \"reason\": \"Security weakening detected in [file]\",\n  \"systemMessage\": \"ðŸ”’ SECURITY REVIEW REQUIRED\\n\\nChange type: [DISABLING/WEAKENING]\\nFile: [path]\\n\\nThis modification appears to reduce security.\\nRequired: Security team review\\n\\nTo proceed:\\n1. Create security review ticket\\n2. Get approval from security team\\n3. Document justification\"\n}\n\nMEDIUM RISK (refactoring, infrastructure):\n{\n  \"decision\": \"approve\",\n  \"systemMessage\": \"âš ï¸ Security-sensitive file modified: [path]\\n\\nChange type: [REFACTORING/INFRASTRUCTURE]\\n\\nReminders:\\n- Run security tests before merge\\n- Request security team review in PR\\n- Update security documentation if needed\"\n}\n\nLOW RISK (strengthening, adding features):\n{\n  \"decision\": \"approve\",\n  \"systemMessage\": \"âœ… Security improvement detected in [path]. Good work!\"\n}\n\nNON-SECURITY (normal files):\n{\n  \"decision\": \"approve\"\n}\n\nCheck if the file path matches any security-sensitive patterns. Evaluate based on the pattern matching and respond with JSON only.",
            "timeout": 25
          }
        ],
        "toolNames": ["Write", "Edit", "MultiEdit"]
      },
      {
        "name": "test-coverage-protection",
        "description": "Prevents test deletion and coverage reduction (Phase 3 - Code Quality)",
        "hooks": [
          {
            "type": "prompt",
            "prompt": "You are a test coverage protection expert evaluating test file modifications.\n\nFile path: $ARGUMENTS.filePath\nOperation: $ARGUMENTS.toolName\n\n**Test File Patterns**:\n- **/*.test.{js,ts,py,jsx,tsx}\n- **/*.spec.{js,ts,py,jsx,tsx}\n- **/tests/**/*\n- **/__tests__/**/*\n- **/test_*.py\n\n**Evaluation Criteria**:\n\n1. **Test Deletion**\n   - Are test files being deleted?\n   - Is this part of feature removal? (OK if feature removed)\n   - Is replacement test being added? (OK if refactoring)\n\n2. **Test Content Modification**\n   - Are test assertions being removed?\n   - Are test cases being deleted without replacement?\n   - Is this test refactoring? (OK if coverage maintained)\n\n3. **Coverage Impact**\n   - Would this reduce overall test coverage?\n   - Are critical paths losing test coverage?\n   - Is this justified by feature removal?\n\n**Decision Logic**:\n\nHIGH RISK - BLOCK:\n{\n  \"decision\": \"block\",\n  \"reason\": \"Test coverage reduction detected\",\n  \"systemMessage\": \"âš ï¸ TEST COVERAGE PROTECTION\\n\\nTest file modification detected: [file]\\n\\nThis change appears to reduce test coverage.\\n\\nIf this is justified (feature removed, test refactored):\\n1. Ensure replacement tests exist\\n2. Verify coverage maintained: run coverage report\\n3. Document reason in commit message\\n\\nIf reducing coverage intentionally:\\n1. Get team approval\\n2. Update coverage thresholds if needed\"\n}\n\nMEDIUM RISK - WARN:\n{\n  \"decision\": \"approve\",\n  \"systemMessage\": \"âš ï¸ Test file modified: [file]\\n\\nPlease verify:\\n- Test coverage maintained or improved\\n- All tests still pass\\n- No critical paths lost coverage\\n\\nRun: coverage report before committing\"\n}\n\nLOW RISK - APPROVE:\n{\n  \"decision\": \"approve\",\n  \"systemMessage\": \"âœ… Test enhancement detected. Good work improving test coverage!\"\n}\n\nNON-TEST FILE:\n{\n  \"decision\": \"approve\"\n}\n\nEvaluate if this is a test file and the impact of the modification. Respond with JSON only.",
            "timeout": 20
          }
        ],
        "toolNames": ["Write", "Edit", "MultiEdit"]
      },
      {
        "name": "dependency-impact-analyzer-llm",
        "description": "Assesses breaking change risk with LLM intelligence (Phase 4 - Hook Enhancement)",
        "hooks": [
          {
            "type": "prompt",
            "prompt": "You are a dependency impact expert assessing breaking change risk for code modifications.\n\nFile being modified: $ARGUMENTS.filePath\nTool: $ARGUMENTS.toolName\nChange context: Analyze the modification for breaking changes\n\n**Analysis Framework**:\n\n1. **API Contract Changes**\n   - Function signature changes (parameters added/removed/reordered)\n   - Return type changes\n   - Behavior changes (exceptions, side effects)\n   - Interface/class contract modifications\n\n2. **Breaking Change Detection**\n   - Are exported functions/classes being modified?\n   - Is backward compatibility maintained?\n   - Are there deprecation warnings needed?\n   - Would existing calls fail?\n\n3. **Risk Assessment**\n   - Low: Internal refactoring, no API changes\n   - Medium: API additions (backward compatible)\n   - High: API modifications (potentially breaking)\n   - Critical: API removals or incompatible changes\n\n4. **Migration Complexity**\n   - Easy: No code changes needed (backward compatible)\n   - Medium: Minor updates (add params, update calls)\n   - Hard: Significant refactoring required\n   - Very Hard: Architecture changes, extensive updates\n\n**Decision Logic**:\n\nCRITICAL RISK - BLOCK:\n{\n  \"decision\": \"block\",\n  \"reason\": \"Critical breaking change detected\",\n  \"systemMessage\": \"ðŸ”´ BREAKING CHANGE DETECTED\\n\\nFile: [filePath]\\nBreaking change type: [specific change]\\n\\nThis change will break existing code that depends on this file.\\n\\nRequired actions BEFORE proceeding:\\n1. Review all dependent files and update calls\\n2. Add deprecation warnings if supporting old API\\n3. Update tests to cover new behavior\\n4. Document breaking change in changelog\\n5. Consider versioning strategy (semver major bump)\\n\\nEstimated migration effort: [time estimate]\\n\\nIf you must proceed without migration:\\n1. Create migration guide\\n2. Notify team of breaking change\\n3. Schedule migration sprint\"\n}\n\nHIGH RISK - WARN:\n{\n  \"decision\": \"approve\",\n  \"systemMessage\": \"ðŸŸ  POTENTIAL BREAKING CHANGE\\n\\nFile: [filePath]\\nChange type: [specific change]\\nRisk: High\\n\\nâš ï¸ This modification may affect dependent code.\\n\\nRecommendations:\\n1. Run full test suite\\n2. Check for deprecated usage patterns\\n3. Review dependent files: [list if known]\\n4. Consider backward compatibility layer\\n\\nMigration complexity: [easy/medium/hard]\\nEstimated affected files: [count if known]\"\n}\n\nMEDIUM RISK:\n{\n  \"decision\": \"approve\",\n  \"systemMessage\": \"ðŸŸ¡ API Modification Detected\\n\\nFile: [filePath]\\nChange: [description]\\n\\nBackward compatible but worth noting:\\n- [Specific change details]\\n\\nVerify:\\n- Tests pass\\n- Documentation updated\\n- No unintended side effects\"\n}\n\nLOW RISK:\n{\n  \"decision\": \"approve\",\n  \"systemMessage\": \"âœ… Safe refactoring detected. No breaking changes.\"\n}\n\nNON-API FILE:\n{\n  \"decision\": \"approve\"\n}\n\nAnalyze the file modification for breaking changes. If file content is not available, make best assessment from file path and context. Respond with JSON only.",
            "timeout": 30
          }
        ],
        "toolNames": ["Edit", "Write", "MultiEdit"]
      }
    ],
    "SubagentStop": [
      {
        "name": "tdd-completion-validator",
        "description": "Validates TDD red-green-refactor cycle completion (Phase 2 - Agent Quality Gates)",
        "hooks": [
          {
            "type": "prompt",
            "prompt": "You are a TDD methodology expert validating test-driven development cycle completion.\n\nAgent: $ARGUMENTS.agentName\nTask: $ARGUMENTS.taskDescription\nExecution context: Analyze the agent's work output\n\n**TDD Cycle Requirements**:\n\n**RED Phase (Failing Test)**:\n1. Was a failing test written FIRST (before implementation)?\n2. Did the test fail for the RIGHT REASON? (not syntax error, not missing import)\n3. Are test assertions SPECIFIC and MEANINGFUL? (not just \"assert result is not None\")\n4. Is the test name DESCRIPTIVE? (e.g., test_user_login_with_valid_credentials)\n\n**GREEN Phase (Minimal Implementation)**:\n1. Was MINIMAL code implemented to make the test pass?\n2. Does the test actually PASS now?\n3. Was over-engineering AVOIDED? (no premature optimization)\n\n**REFACTOR Phase (Code Improvement)**:\n1. Was code refactored for clarity/quality?\n2. Do all tests STILL PASS after refactoring?\n3. Was test coverage MAINTAINED or IMPROVED?\n\n**Coverage & Quality Gates**:\n1. Line coverage >= 80% OR delta coverage = 100%\n2. Branch coverage >= 75%\n3. Critical path coverage = 100% (if applicable)\n4. Mutation score >= 85% (if mutation testing was mentioned)\n5. No test smells (brittle assertions, test interdependence)\n\n**Completion Checklist**:\nâœ“ All three phases completed (RED â†’ GREEN â†’ REFACTOR)\nâœ“ Coverage thresholds met\nâœ“ Tests are high quality\nâœ“ No failing tests\nâœ“ Code quality maintained\n\n**Decision Logic**:\n\nBLOCK if:\n- RED phase skipped (test written after implementation)\n- Test didn't actually fail initially\n- Coverage below threshold (<80% line, <75% branch)\n- Tests currently failing\n- Critical path not covered\n\nAPPROVE if:\n- All three phases completed\n- Coverage meets or exceeds thresholds\n- Test quality is high\n- All tests passing\n\n**Response Format**:\n\nIncomplete cycle:\n{\n  \"decision\": \"block\",\n  \"reason\": \"TDD cycle incomplete\",\n  \"stopReason\": \"âŒ TDD Cycle Incomplete\\n\\nMissing or incomplete:\\n[List specific items]\\n\\nRequired actions:\\n- [Action 1]\\n- [Action 2]\\n\\nCurrent coverage: XX% (target: 80% line, 75% branch)\\nCurrent phase: [which phase incomplete]\"\n}\n\nComplete cycle:\n{\n  \"decision\": \"approve\",\n  \"reason\": \"TDD cycle complete with quality gates passed\",\n  \"systemMessage\": \"âœ… TDD Cycle Complete\\n\\nRED: [test name] failed correctly\\nGREEN: Minimal implementation, test passes\\nREFACTOR: Code improved, tests still passing\\n\\nCoverage: XX% line, XX% branch âœ…\\nMutation score: XX% âœ…\\n\\nGreat work maintaining TDD discipline!\"\n}\n\nAnalyze the TDD workflow and respond with JSON only.",
            "timeout": 30
          }
        ],
        "agentNames": ["tdd-orchestrator", "tdd-python", "tdd-typescript"]
      },
      {
        "name": "security-analysis-completion",
        "description": "Validates comprehensive OWASP Top 10 security audit (Phase 2 - Agent Quality Gates)",
        "hooks": [
          {
            "type": "prompt",
            "prompt": "You are a security audit expert validating security analysis completeness.\n\nAgent: $ARGUMENTS.agentName\nScope: $ARGUMENTS.taskDescription\nFindings: Analyze the security audit output\n\n**OWASP Top 10 Coverage Checklist**:\n\nRequired Categories (must check ALL 10):\n\n1. A01: Broken Access Control (IDOR, missing auth, privilege escalation)\n2. A02: Cryptographic Failures (weak algorithms, hardcoded secrets, insecure TLS)\n3. A03: Injection (SQL, XSS, command injection)\n4. A04: Insecure Design (missing rate limiting, business logic flaws)\n5. A05: Security Misconfiguration (default credentials, verbose errors)\n6. A06: Vulnerable Components (outdated dependencies, known CVEs)\n7. A07: Authentication Failures (weak passwords, missing MFA, session issues)\n8. A08: Data Integrity Failures (insecure deserialization, missing integrity checks)\n9. A09: Security Logging Failures (missing audit logs, logging sensitive data)\n10. A10: Server-Side Request Forgery (SSRF via user URLs, internal access)\n\n**Critical Findings Checklist**:\nâœ“ All Critical (CVSS 9.0+) vulnerabilities identified\nâœ“ Specific remediations provided for each finding\nâœ“ CVSS scores calculated\nâœ“ Exploit scenarios documented\nâœ“ Compliance mapping (PCI DSS, GDPR, etc. if applicable)\n\n**Secret Detection**:\nâœ“ Scanned for hardcoded credentials\nâœ“ Checked for API keys/tokens\nâœ“ Reviewed for private keys\n\n**Dependency Security**:\nâœ“ Ran dependency scanner (npm audit, pip-audit, etc.)\nâœ“ Identified vulnerable packages\nâœ“ Provided upgrade paths\n\n**Decision Logic**:\n\nBLOCK if:\n- Any OWASP category skipped (incomplete audit)\n- Critical vulnerabilities found but unresolved\n- No CVSS scores provided\n- Secrets detected but not addressed\n- Dependency scan not run\n\nAPPROVE if:\n- All 10 OWASP categories checked\n- Critical findings have remediations\n- CVSS scoring complete\n- Comprehensive report delivered\n\n**Response Format**:\n\nIncomplete audit:\n{\n  \"decision\": \"block\",\n  \"reason\": \"Security audit incomplete\",\n  \"stopReason\": \"ðŸ”’ Security Audit Incomplete\\n\\nMissing:\\n- [OWASP categories not covered]\\n- [Critical findings without remediation]\\n- [Required scans not run]\\n\\nRequired actions:\\n1. Complete OWASP Top 10 scan (X/10 done)\\n2. Provide CVSS scores for all findings\\n3. Document remediation steps for critical issues\\n4. Run dependency security scan\"\n}\n\nComplete audit:\n{\n  \"decision\": \"approve\",\n  \"reason\": \"Comprehensive security audit complete\",\n  \"systemMessage\": \"âœ… Security Audit Complete\\n\\nOWASP Top 10: 10/10 âœ…\\nFindings: X Critical, X High, X Medium, X Low\\nCVSS Scoring: Complete âœ…\\nRemediation Plans: Complete âœ…\\n\\nSecurity Score: XX/100\\nRecommendation: [Ready for production / Requires fixes before deployment]\"\n}\n\nAnalyze the security audit and respond with JSON only.",
            "timeout": 30
          }
        ],
        "agentNames": ["security-analyzer"]
      },
      {
        "name": "debug-resolution-validator",
        "description": "Validates bug fix with root cause analysis and prevention (Phase 2 - Agent Quality Gates)",
        "hooks": [
          {
            "type": "prompt",
            "prompt": "You are a debugging expert validating bug fix completion and quality.\n\nAgent: $ARGUMENTS.agentName\nBug: $ARGUMENTS.taskDescription\nResolution: Analyze the debugging process and fix\n\n**Root Cause Analysis (5 Whys)**:\n\nWas root cause analysis performed?\nâœ“ Surface symptom identified\nâœ“ Immediate cause identified\nâœ“ Underlying cause identified\nâœ“ Root cause identified (answer to \"Why #5\")\nâœ“ Contributing factors identified\n\nExample good RCA:\n```\nSymptom: User registration fails with 500 error\nWhy 1: Database constraint violation\nWhy 2: Duplicate email addresses\nWhy 3: Multiple form submissions\nWhy 4: Submit button not disabled\nWhy 5: Missing client-side debouncing\n\nRoot Cause: Frontend missing submit button debounce\n```\n\n**Test-Driven Debugging**:\n\n1. Was a failing test created to reproduce the bug?\n2. Did the test initially fail (confirms reproduction)?\n3. Was the fix applied?\n4. Does the test now pass?\n5. Does the full test suite still pass (no regressions)?\n\n**Fix Quality**:\n\nâœ“ Addresses ROOT CAUSE (not just symptom)\nâœ“ Minimal change (surgical fix)\nâœ“ No side effects introduced\nâœ“ Code quality maintained\nâœ“ Documentation updated\n\n**Prevention Strategy**:\n\nWere prevention measures implemented?\n1. Regression test added (test_[bug_description])\n2. Monitoring/alerting added (if production bug)\n3. Input validation improved (if applicable)\n4. Error handling enhanced (if applicable)\n5. Documentation/runbook updated\n\n**Decision Logic**:\n\nBLOCK if:\n- No root cause analysis performed (just symptom fix)\n- No failing test created to reproduce bug\n- Tests still failing\n- Regression in test suite\n- No prevention strategy documented\n\nAPPROVE if:\n- Root cause identified (5 Whys completed)\n- Failing test created and now passes\n- Full test suite passes\n- Prevention measures documented\n- Fix addresses root cause\n\n**Response Format**:\n\nIncomplete fix:\n{\n  \"decision\": \"block\",\n  \"reason\": \"Bug fix incomplete or lacks quality\",\n  \"stopReason\": \"âŒ Bug Fix Incomplete\\n\\nMissing:\\n- [Root cause analysis (5 Whys)]\\n- [Failing test to reproduce bug]\\n- [Prevention strategy]\\n- [Tests still failing: X tests]\\n\\nRequired actions:\\n1. Perform 5 Whys root cause analysis\\n2. Create failing test: test_[bug_description]\\n3. Verify all tests pass\\n4. Document prevention (monitoring, validation, etc.)\\n\\nCurrent: [Symptom fixed / Root cause unknown / Tests failing]\"\n}\n\nComplete fix:\n{\n  \"decision\": \"approve\",\n  \"reason\": \"Bug fix complete with root cause analysis and prevention\",\n  \"systemMessage\": \"âœ… Bug Fix Complete\\n\\nRoot Cause: [identified via 5 Whys]\\nFix: [description of fix]\\nTest: test_[name] âœ… (created and passing)\\nRegression Suite: All passing âœ…\\nPrevention: [measures taken]\\n\\nBug resolved and won't recur. Excellent debugging work!\"\n}\n\nAnalyze the bug fix workflow and respond with JSON only.",
            "timeout": 30
          }
        ],
        "agentNames": ["smart-debug"]
      },
      {
        "name": "code-quality-completion",
        "description": "Validates comprehensive code quality analysis completion (Phase 3 - Code Quality)",
        "hooks": [
          {
            "type": "prompt",
            "prompt": "You are a code quality expert validating analysis completion.\n\nAgent: $ARGUMENTS.agentName\nMode: $ARGUMENTS.taskDescription\nFindings: Analyze the code quality analysis output\n\n**Mode-Specific Requirements**:\n\n**Security Review Mode**:\nâœ“ Vulnerabilities identified with severity (Critical/High/Medium/Low)\nâœ“ OWASP coverage mentioned\nâœ“ Specific fixes provided\nâœ“ Security scorecard present\n\n**Clarity Refactoring Mode**:\nâœ“ Complexity analysis (cyclomatic complexity, code smells)\nâœ“ Before/after code examples\nâœ“ Refactoring patterns applied (guard clauses, symmetry, cohesion)\nâœ“ Readability improvements documented\n\n**Synthesis Analysis Mode**:\nâœ“ Cross-file dependencies mapped\nâœ“ API consistency checked\nâœ“ Architectural patterns validated\nâœ“ Integration issues identified\n\n**General Quality Checks** (All Modes):\n\n1. **Issue Identification**\n   - All critical issues have file location + line numbers\n   - Issues prioritized by severity (Critical â†’ Low)\n   - Specific, not vague descriptions\n\n2. **Fix Recommendations**\n   - Every issue has specific fix recommendation\n   - Before/after examples provided for clarity issues\n   - Fixes preserve functionality (tests mentioned)\n\n3. **Metrics & Scoring**\n   - Complexity metrics provided (if relevant)\n   - Security scores calculated (if security mode)\n   - Quality assessment quantified\n\n4. **Validation**\n   - Tests mentioned (if fixes applied)\n   - No regressions introduced\n   - Code quality maintained or improved\n\n**Decision Logic**:\n\nBLOCK if:\n- No issues identified when obvious problems exist\n- Critical issues without specific fixes\n- No file locations or line numbers\n- No prioritization (everything \"Critical\" or no severity)\n- Mode-specific requirements not met\n\nAPPROVE if:\n- Comprehensive analysis delivered\n- All critical issues have fixes\n- Prioritization clear\n- Mode-specific requirements met\n- Actionable recommendations provided\n\n**Response Format**:\n\nIncomplete analysis:\n{\n  \"decision\": \"block\",\n  \"reason\": \"Code quality analysis incomplete\",\n  \"stopReason\": \"âŒ Code Quality Analysis Incomplete\\n\\nMissing:\\n- [Mode-specific requirements not met]\\n- [Critical issues without fixes]\\n- [Missing file locations/line numbers]\\n\\nRequired actions:\\n1. Complete [Security/Clarity/Synthesis] analysis\\n2. Provide specific fixes for all critical issues\\n3. Include file locations and line numbers\\n4. Prioritize issues by severity\\n\\nCurrent: [What's incomplete]\"\n}\n\nComplete analysis:\n{\n  \"decision\": \"approve\",\n  \"reason\": \"Comprehensive code quality analysis complete\",\n  \"systemMessage\": \"âœ… Code Quality Analysis Complete\\n\\nMode: [Security/Clarity/Synthesis]\\nIssues Found: X Critical, X High, X Medium, X Low\\nFixes Provided: Complete âœ…\\nPrioritization: Clear âœ…\\n\\nQuality Score: XX/100\\nRecommendation: [Specific next steps]\"\n}\n\nAnalyze the code quality review and respond with JSON only.",
            "timeout": 30
          }
        ],
        "agentNames": ["code-quality-analyzer"]
      }
    ],
    "Stop": [
      {
        "name": "work-completion-validator",
        "description": "Context-aware work completion validation (Phase 3 - Code Quality)",
        "hooks": [
          {
            "type": "prompt",
            "prompt": "You are a work completion expert providing context-aware quality assessment.\n\n**Context-Aware Evaluation**:\n\n1. **Intent Alignment**\n   - Did we actually accomplish what the user requested?\n   - Is the work functionally complete?\n   - Are there obvious gaps in the solution?\n\n2. **Quality Assessment**\n   - Is the code/solution production-ready?\n   - Are TODOs acceptable?\n     * \"TODO: future feature XYZ\" = OK (not current scope)\n     * \"TODO: fix this hack\" = NOT OK (current work incomplete)\n     * \"TODO: optimize later\" = OK if working correctly\n   - Are uncommitted changes intentional?\n     * \"WIP: experimenting with approach\" = OK (documented)\n     * \"Forgot to commit working code\" = NOT OK (should commit)\n\n3. **Testing & Validation**\n   - Did tests run and pass? (required for production code)\n   - Is code formatted/linted? (required for team standards)\n   - Are there obvious bugs or issues?\n\n4. **Documentation**\n   - Is documentation updated? (required if APIs/interfaces changed)\n   - Are breaking changes documented?\n   - Is usage clear for future developers?\n\n5. **Risk Evaluation**\n   - Are there unaddressed edge cases?\n   - Is error handling sufficient?\n   - Are security implications considered?\n\n**Decision Logic**:\n\nBLOCK if:\n- User request not actually fulfilled\n- Critical TODOs remain (\"fix this hack\", \"security issue\", \"broken\")\n- Tests failing\n- Production code without any tests\n- Obvious quality/functionality issues\n- Breaking changes without documentation\n\nWARN (approve with recommendations) if:\n- Minor improvements possible\n- Documentation could be better (but APIs unchanged)\n- Test coverage could be higher (but meets minimum)\n- Future TODOs present (not blocking)\n\nAPPROVE if:\n- User request fulfilled\n- Quality acceptable for context\n- No critical issues\n- Future TODOs clearly marked as future work\n- Tests pass (if code changed)\n\n**Response Format**:\n\nIncomplete work:\n{\n  \"decision\": \"block\",\n  \"reason\": \"Work incomplete or quality issues\",\n  \"stopReason\": \"âŒ Work Incomplete\\n\\n[Specific issues with context]\\n\\nThe work doesn't fully address: [user request]\\n\\nRequired actions:\\n- [Specific action 1]\\n- [Specific action 2]\\n\\nContext: [Why this matters]\"\n}\n\nComplete with recommendations:\n{\n  \"decision\": \"approve\",\n  \"systemMessage\": \"âœ… Work Complete\\n\\nâš ï¸ Recommendations for future:\\n- [Optional improvement 1]\\n- [Optional improvement 2]\\n\\nThese can be addressed in future PRs if needed.\"\n}\n\nComplete:\n{\n  \"decision\": \"approve\",\n  \"reason\": \"Work complete and quality verified\",\n  \"systemMessage\": \"âœ… Work Complete\\n\\n[Brief summary of accomplishments]\\nQuality: Good\\nTests: Passing\\nDocumentation: Updated\"\n}\n\nNow evaluate work completion with context awareness. Respond with JSON only.",
            "timeout": 30
          }
        ]
      }
    ],
    "PostToolUse": [
      {
        "name": "code-narrator-llm",
        "description": "Generates context-aware natural language narratives of code changes (Phase 4 - Hook Enhancement)",
        "hooks": [
          {
            "type": "prompt",
            "prompt": "You are a code change narrator expert generating plain-English explanations of code modifications.\n\nFile: $ARGUMENTS.filePath\nTool: $ARGUMENTS.toolName\nModification context: Analyze what changed and why\n\n**Your Task**:\n\nGenerate a concise, insightful narrative that explains:\n\n1. **WHAT changed**\n   - Specific functions, classes, logic modified\n   - Dependencies added or removed\n   - File structure changes\n\n2. **WHY the change was made** (infer intent)\n   - Bug fix, feature addition, refactoring, performance improvement?\n   - What problem does this solve?\n   - What value does this provide?\n\n3. **IMPACT of the change**\n   - Who needs to know about this?\n   - What should be tested?\n   - Are there risks or side effects?\n\n4. **Stakeholder-specific insights**\n   - PM: Business impact (new feature, improvement, fix)\n   - QA: Testing recommendations (what to verify)\n   - Docs: Documentation updates needed (API changes, new features)\n   - Dev: Technical details (architecture, patterns, dependencies)\n\n**Guidelines**:\n- Be concise but insightful (2-4 sentences for narrative)\n- Avoid jargon unless necessary\n- Focus on intent and business value, not just mechanics\n- Provide actionable recommendations\n- Use context clues from code to infer purpose\n\n**Response Format** (JSON only):\n\n{\n  \"narrative\": \"Plain English summary explaining what, why, and impact\",\n  \"intent\": \"bug_fix|feature_addition|refactoring|performance|documentation|security\",\n  \"key_changes\": [\n    \"Specific change 1\",\n    \"Specific change 2\"\n  ],\n  \"stakeholder_impact\": {\n    \"pm\": \"Business/product impact\",\n    \"qa\": \"What to test and verify\",\n    \"docs\": \"Documentation updates needed\",\n    \"dev\": \"Technical implementation details\"\n  },\n  \"testing_recommendation\": \"Specific testing guidance\",\n  \"risk_level\": \"low|medium|high\",\n  \"confidence\": \"high|medium|low\"\n}\n\nAnalyze the code change and respond with JSON only. Be specific and actionable.",
            "timeout": 25
          }
        ],
        "toolNames": ["Edit", "Write", "MultiEdit"]
      },
      {
        "name": "similar-code-finder-llm",
        "description": "Finds semantically similar code using LLM semantic analysis (Phase 4 - Hook Enhancement)",
        "hooks": [
          {
            "type": "prompt",
            "prompt": "You are a code similarity expert identifying semantic duplication and refactoring opportunities using intelligent analysis.\n\nModified file: $ARGUMENTS.filePath\nTool: $ARGUMENTS.toolName\nContext: Analyze for semantic similarity and refactoring\n\n**Your Task**:\n\nIdentify semantic duplication and refactoring opportunities by analyzing:\n\n1. **True Semantic Duplicates**\n   - Functionally equivalent code (same intent, different syntax)\n   - Similar algorithms with different implementations\n   - Copy-pasted code with minor variations\n   - Parallel implementations that should be unified\n\n2. **Refactoring Opportunities**\n   - Common patterns that could be extracted\n   - Repeated logic that could be shared\n   - Inconsistent implementations of same concept\n   - Code that violates DRY principle\n\n3. **Pattern Analysis**\n   - What design patterns are present?\n   - Are patterns applied consistently?\n   - Could better patterns be used?\n   - Are there anti-patterns?\n\n4. **Specific Recommendations**\n   - Which files should be refactored together?\n   - What should be extracted (function, class, module)?\n   - How difficult is the refactoring?\n   - What's the ROI (effort vs. maintainability gain)?\n\n**Guidelines**:\n- Focus on semantic similarity, not just text matching\n- Be specific: provide file paths, function names, line references\n- Prioritize by impact and effort\n- Consider maintainability and code clarity\n- Only suggest refactoring if it adds clear value\n\n**Response Format** (JSON only):\n\n{\n  \"semantic_duplicates\": [\n    {\n      \"file\": \"path/to/similar/file.py\",\n      \"location\": \"function_name() at line X\",\n      \"similarity_reason\": \"Both implement X algorithm using Y pattern\",\n      \"differences\": \"Minor variations in error handling\",\n      \"confidence\": \"high|medium|low\"\n    }\n  ],\n  \"refactoring_opportunities\": [\n    {\n      \"pattern\": \"Extract common validation logic\",\n      \"affected_files\": [\"file1.py\", \"file2.py\"],\n      \"suggested_refactoring\": \"Create shared validator utility\",\n      \"location\": \"Specific functions or classes\",\n      \"effort\": \"low|medium|high\",\n      \"roi\": \"high|medium|low\",\n      \"priority\": \"high|medium|low\"\n    }\n  ],\n  \"patterns_detected\": [\n    {\n      \"pattern\": \"Strategy pattern\",\n      \"files\": [\"file1.py\", \"file2.py\"],\n      \"consistency\": \"consistent|inconsistent\",\n      \"recommendation\": \"Continue using or improve consistency\"\n    }\n  ],\n  \"overall_assessment\": \"Brief summary of code duplication state and recommendations\",\n  \"action_required\": true|false\n}\n\nAnalyze for semantic similarity and respond with JSON only. Be specific and actionable. If no significant duplicates or opportunities found, return empty arrays with overall_assessment explaining this.",
            "timeout": 25
          }
        ],
        "toolNames": ["Edit", "MultiEdit"]
      }
    ],
    "UserPromptSubmit": [
      {
        "name": "test-data-generator-llm",
        "description": "Generates domain-aware, contextually appropriate test data using LLM (Phase 4 - Hook Enhancement)",
        "hooks": [
          {
            "type": "prompt",
            "prompt": "You are a test data generation expert creating realistic, domain-appropriate test data based on user intent and project context.\n\nUser prompt: $ARGUMENTS.prompt\n\n**Your Task**:\n\nGenerate intelligent, context-aware test data by:\n\n1. **Understanding Intent**\n   - What is the user trying to test?\n   - What domain does this belong to (e-commerce, finance, healthcare, etc.)?\n   - What entities are involved (user, product, order, payment, etc.)?\n   - What scenarios need testing (happy path, edge cases, errors)?\n\n2. **Domain-Aware Data Generation**\n   - Use domain-appropriate values (e.g., realistic product names for e-commerce)\n   - Maintain internal consistency (e.g., order totals match item prices)\n   - Follow domain constraints (e.g., valid credit card formats, legal age restrictions)\n   - Use project-specific terminology if detectable\n\n3. **Scenario-Specific Edge Cases**\n   - What edge cases matter for THIS specific test?\n   - What failure modes should be tested?\n   - What boundary conditions exist?\n   - What security concerns apply?\n\n4. **Format and Structure**\n   - Detect programming language (Python, JavaScript, TypeScript, etc.)\n   - Format data appropriately for the language\n   - Provide ready-to-use code snippets\n   - Include import statements if needed\n\n**Guidelines**:\n- Be realistic and internally consistent\n- Match the domain and context\n- Provide scenario-specific edge cases, not generic ones\n- Make data copy-paste ready\n- Include usage examples\n- Consider test isolation (deterministic values when appropriate)\n\n**Response Format** (JSON only):\n\n{\n  \"test_context\": {\n    \"intent\": \"What user is trying to test\",\n    \"domain\": \"ecommerce|finance|healthcare|social|education|generic\",\n    \"entities\": [\"user\", \"product\", \"order\"],\n    \"scenario\": \"happy_path|edge_cases|error_handling|integration\",\n    \"language\": \"python|javascript|typescript|java|go\"\n  },\n  \"test_data\": {\n    \"description\": \"Brief description of test data\",\n    \"code\": \"Ready-to-use code snippet with test data\",\n    \"format\": \"json|python_dict|js_object|etc\"\n  },\n  \"edge_cases\": [\n    {\n      \"scenario\": \"Specific edge case name\",\n      \"data\": \"Edge case test data\",\n      \"why\": \"Why this edge case matters\"\n    }\n  ],\n  \"usage_example\": \"Code snippet showing how to use the test data\",\n  \"testing_tips\": [\n    \"Specific tip 1\",\n    \"Specific tip 2\"\n  ],\n  \"rationale\": \"Why this test data is appropriate for the scenario\"\n}\n\nAnalyze the user prompt and generate appropriate test data. Respond with JSON only. If prompt is not test-related, set test_context.intent to 'not_applicable' and skip data generation.",
            "timeout": 25
          }
        ]
      }
    ]
  },
  "extraKnownMarketplaces": {
    "grey-haven-plugins": {
      "source": {
        "source": "directory",
        "path": "/home/user/claude-code-config/grey-haven-plugins"
      }
    }
  },
  "enabledPlugins": {
    "agent-orchestration@grey-haven-plugins": true,
    "browser-automation@grey-haven-plugins": true,
    "cc-trace@grey-haven-plugins": true,
    "cloudflare-deployment-observability@grey-haven-plugins": true,
    "core@grey-haven-plugins": true,
    "data-quality@grey-haven-plugins": true,
    "deployment@grey-haven-plugins": true,
    "developer-experience@grey-haven-plugins": true,
    "incident-response@grey-haven-plugins": true,
    "knowledge-base@grey-haven-plugins": true,
    "linear@grey-haven-plugins": true,
    "observability@grey-haven-plugins": true,
    "research@grey-haven-plugins": true,
    "security@grey-haven-plugins": true,
    "testing@grey-haven-plugins": true
  }
}
